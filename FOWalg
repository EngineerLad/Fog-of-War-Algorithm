'''
"Fog-of-War" Machine Learning Algorithm
Takes pre-formatted input and output data to train a model based on spherical Gaussian probability distribution(created using interpolation) in 3 dimensions.
Conceptually, it is a distribution-based clustering algorithm. However, this methodology takes advantage of multi-nodal analysis, 
dynamically expanding centroid area and creating multiple centroid nodes for a more robust algorithm than simply calculating the distance from the calculated centroid based off collected data.
This allows the program to work with less data points while training and still give viable data outputs in the testing and application phase.
The dynamic multi-layered approach allows the program to withstand high-bias, small-size datasets, thus having a high accuracy to data saturation ratio, making it more resilient against overfitting.

Here is a step-by-step breakdown of how this algorithm works:

1. After thorough data formatting, the algorithm receives an array of training data that contains both the conditional variables in the input and the resulting output. (The minimum data point requirement is the number of unique outputs)
2. It performs a data profiling step, which takes the number of variables in the data input array and sets the number of variables that need to be tracked throughout the program's run.
3. Each data point contributes to a general function via compound average probability that directly correlates to a probability field per data input. Each input variable will have one generated function per possible output category.
4. Once the training is complete, the program is left with a general function with the input being the input value and the output is the amplitude of the function given the input value.
5. The output value is divided by the global maximum of that function to get the probability of that input belonging to part of the output's general data profile.
6. Each of those probabilities are cross examined across within each other's same variable types, then the predicted output types for each variable is cross examined with each other to see which output type has the most qualities fulfilled to reach a conclusion.

'''

import numpy as np
import matplotlib.pyplot as plt
from scipy import interpolate


class fogofwar():
    def __init__(self, instanceNumber):
        algvar = {
            #numinputvar = number of input variables per datapoint
            "numinputvar" : 0, 
            #numuniqueout = number of unique outcomes based on data profiling
            "numuniqueout" : 0,
            #minslopemag = the minimum slope magnitude between data input values in the general function before it is considered a separate node instead of a wider range for the same characteristic. 
            # This is the function that will readjust automatically given cost values. Each slope has a default slope of 1, and there is one slope for each input
            "minslopemags" : [],
            #dynlr = dynamic learning rate. Calculated using the difference in cost per iteration. If the rate of change in cost is positive and increasing, the value increases. If it is instead negative and decreasing, the value decreases.
            # If it is negative/increasing or positive/decreasing, it will remain constant. Learning rates will be a randn generated at initialization.
            "dynlr" : [],
            #costvalues = a list of cost values per general equation. Past one point of data per unique output, the program can make a prediction based on the existing general functions. 
            # Based on the performance per input variable type(the probabilities for every variation of one input variable for each unique output), it will adjust the minimum slope magnitudes and amplitudes accordingly.
            # The cost is calculated by taking the inverse difference of the probability of other functions and the probability of the actual. It is then multiplied by the dynamic learning rate and that determines the change in slope for that iteration.
            "costvalues" : [],
            #generaleq = general equations for the model. stored here after training has ended and retrieved for use here.
            "generaleq":[]



            }
    #hazetrain = function that takes a list of datapoints formatted as such: [[input variables here, output variable here],datapoint2,...]. It then processes the information of the given dataset as explained above.
    def deMistify(self,data):
        #gets number of input variables per datapoint
        numinputvar = len(data[0])
        self.algvar["numinputvar"] = numinputvar
        #gets number of unique outputs for the dataset
        outputlist = []
        for i in range(len(data)):
            outputlist.append(data[i][-1])
        uniqueoutputs = np.unique(outputlist)
        numuniqueout = len(uniqueoutputs)
        self.algvar["numuniqueout"] = numuniqueout
        #gets maximum number of decimal points to work with (significant figures, so takes the smallest number of decimal places and rounds the rest of the values based on that and determines the data)
        smallest = 1000000 #kekw i need to work on how to set this value properly
        for i in range(len(data)):
            for j in range(len(data[0])):
                sigfigcount = str(a)[::-1].find('.')
                if sigfigcount < smallest:
                    smallest = sigfigcount
        #rounds to the sigfig for each value
        for i in range(len(data)):
            for j in range(len(data[0])):
                data[i][j] = round(data[i][j],smallest)


               

        #building a list of the initiation data points from the original data list to start the equations out
        initdatapoint = []

        for i in range(len(data)):
            if data[i][-1] in uniqueoutputs:
                #adds the point containing the output label
                initdatapoint.append(data[i])
                #then removes the point to avoid confusion
                #aka a reminder to MAKE THIS MORE EFFICIENT REEEEEEEEEEE
                uniqueoutputs.remove(data[i][-1])
                
        #remove the points used for the initial data points from the rest of the training dataset
        remainderpoints = data
        for i in range(len(initdatapoint)):
            remainderpoints.remove(initdatapoint[i])

        

        #generate general functions:
        generaleq = []
        eqgroup = []
        #use scipy to make an interpolated spline function for the general solution function. The number of equations needed is the product of the number of inputs and the number of unique outputs.
        #Start with the first datapoint per output as the foundation.
        for i in range(len(initdatapoint)):
            for j in range(len(initdatapoint[i])-1):
                x = np.linspace(0,int(initdatapoint[i][j])*2,int(initdatapoint[i][j])*2-1)
                y = [0] * int(initdatapoint[i][j])*2
                y[int(initdatapoint[i][j])-1] = 1
                function = interpolate.interp1d(x,y,kind = 'cubic')
                eqgroup.append(function)
            generaleq.append(eqgroup)
            eqgroup = []

        #Then start analyzing other points
        dpprobs = []
        for i in range(len(remainderpoints)):
            for j in range(len(remainderpoints[i])-1):
                probabilitylist=[]
                for k in range(generaleq):
                    probabilitylist.append(generaleq[k][j][data[i][j]])
                max_prob = max(probabilitylist)
                likelyoutcome = uniqueoutputs[probabilitylist.index(max_prob)]
                if likelyoutcome != data[i][-1]:

